{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dependencies\n",
    "\n",
    "from splinter import Browser\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "executable_path = {'executable_path': '/usr/local/bin/chromedriver'}\n",
    "browser = Browser('chrome', **executable_path, headless=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrape book data and load into a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'http://books.toscrape.com/'\n",
    "browser.visit(url)\n",
    "browser_url = browser.url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize empty lists for storing scraped book titles, links, product descriptions, prices\n",
    "titles = []\n",
    "full_links = []\n",
    "product_descriptions = []\n",
    "prices = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/BootCamp/anaconda3/lib/python3.7/site-packages/splinter/driver/webdriver/__init__.py:536: FutureWarning: browser.find_link_by_text is deprecated. Use browser.links.find_by_text instead.\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping Complete\n"
     ]
    }
   ],
   "source": [
    "# Iterate through all pages\n",
    "for x in range(50):\n",
    "    # HTML object\n",
    "    html = browser.html\n",
    "    # Parse HTML with Beautiful Soup\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    # Retrieve all elements that contain book information\n",
    "    articles = soup.find_all('article', class_='product_pod')\n",
    "    browser_url = '/'.join(browser.url.rstrip('/').split('/')[:-1])\n",
    "\n",
    "    # Iterate through each book\n",
    "    for article in articles:\n",
    "        # Use Beautiful Soup's find() method to navigate and retrieve the anchor tag pertaining to each book\n",
    "        h3 = article.find('h3')\n",
    "        link = h3.find('a')\n",
    "\n",
    "        # Compile the book-specific web address, and handle the landing page's peculiar formatting\n",
    "        href = link['href']\n",
    "        if x == 0:\n",
    "            href = \"books.toscrape.com/\" + href\n",
    "        \n",
    "        # Complete the concatenation of the book page url\n",
    "        full_link = browser_url + \"/\" + href\n",
    "        full_links.append(full_link)\n",
    "\n",
    "        # Retrieve the title of the book and add it to our list of books\n",
    "        title = link['title']\n",
    "        titles.append(title)\n",
    "        \n",
    "\n",
    "    # Click the 'Next' button on each page, otherwise print that scraping is complete\n",
    "    try:\n",
    "        browser.click_link_by_text('next')\n",
    "          \n",
    "    except:\n",
    "        print(\"Scraping Complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aborted attempt to grab the star rating for a particular title\n",
    "\n",
    "#browser.visit(\"http://books.toscrape.com/catalogue/frankenstein_20/index.html\")\n",
    "#html = browser.html\n",
    "#soup = BeautifulSoup(html, 'html.parser')\n",
    "#star_thing = soup.find_all('div', class_ = \"col-sm-6 product_main\")\n",
    "\n",
    "#star_p = soup.find_all('p')\n",
    "\n",
    "#rating_p = star_p[2]\n",
    "\n",
    "#rating_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Follow each book's link to grab the production description and price for each book; store if successful\n",
    "for link in full_links:\n",
    "    try:\n",
    "        browser.visit(link)\n",
    "        html = browser.html\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "        # Find and append the product description of the current book to our list\n",
    "        product_description = soup.find_all('p')[3].text\n",
    "        product_descriptions.append(product_description)\n",
    "        \n",
    "        # Find and append the float-formatted price of the current book to our list\n",
    "        price = float(soup.find_all('p', class_='price_color')[0].text.strip('£'))\n",
    "        prices.append(price)\n",
    "        \n",
    "    except:\n",
    "        print(f\"Page not found at address: {link}\")\n",
    "        product_descriptions.append(\"Description not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe containins the scraped book data (urls, titles, descriptions, and prices)\n",
    "books_df = pd.DataFrame(\n",
    "    {\"link\": full_links,\n",
    "     \"title\": titles,\n",
    "     \"description\": product_descriptions,\n",
    "     \"price\": prices\n",
    "     }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>link</th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>http://books.toscrape.com/catalogue/a-light-in...</td>\n",
       "      <td>A Light in the Attic</td>\n",
       "      <td>It's hard to imagine a world without A Light i...</td>\n",
       "      <td>51.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>http://books.toscrape.com/catalogue/tipping-th...</td>\n",
       "      <td>Tipping the Velvet</td>\n",
       "      <td>\"Erotic and absorbing...Written with starling ...</td>\n",
       "      <td>53.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>http://books.toscrape.com/catalogue/soumission...</td>\n",
       "      <td>Soumission</td>\n",
       "      <td>Dans une France assez proche de la nôtre, un h...</td>\n",
       "      <td>50.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>http://books.toscrape.com/catalogue/sharp-obje...</td>\n",
       "      <td>Sharp Objects</td>\n",
       "      <td>WICKED above her hipbone, GIRL across her hear...</td>\n",
       "      <td>47.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>http://books.toscrape.com/catalogue/sapiens-a-...</td>\n",
       "      <td>Sapiens: A Brief History of Humankind</td>\n",
       "      <td>From a renowned historian comes a groundbreaki...</td>\n",
       "      <td>54.23</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                link  \\\n",
       "0  http://books.toscrape.com/catalogue/a-light-in...   \n",
       "1  http://books.toscrape.com/catalogue/tipping-th...   \n",
       "2  http://books.toscrape.com/catalogue/soumission...   \n",
       "3  http://books.toscrape.com/catalogue/sharp-obje...   \n",
       "4  http://books.toscrape.com/catalogue/sapiens-a-...   \n",
       "\n",
       "                                   title  \\\n",
       "0                   A Light in the Attic   \n",
       "1                     Tipping the Velvet   \n",
       "2                             Soumission   \n",
       "3                          Sharp Objects   \n",
       "4  Sapiens: A Brief History of Humankind   \n",
       "\n",
       "                                         description  price  \n",
       "0  It's hard to imagine a world without A Light i...  51.77  \n",
       "1  \"Erotic and absorbing...Written with starling ...  53.74  \n",
       "2  Dans une France assez proche de la nôtre, un h...  50.10  \n",
       "3  WICKED above her hipbone, GIRL across her hear...  47.82  \n",
       "4  From a renowned historian comes a groundbreaki...  54.23  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preview the books DataFrame\n",
    "books_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrape quote data and load into DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Point the scraper at the quotes site\n",
    "quotes_url = 'http://quotes.toscrape.com/'\n",
    "browser.visit(quotes_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty list for storing speaker names\n",
    "speakers = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through each quote\n",
    "for x in range(1, 11):\n",
    "\n",
    "    \n",
    "    html = browser.html\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "    quotes = soup.find_all('small', class_='author')\n",
    "\n",
    "    for quote in quotes:\n",
    "        speakers.append(quote.text)\n",
    "\n",
    "    if x != 10:\n",
    "        browser.click_link_by_partial_text('Next')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe containing the scraped quotes data (speakers)\n",
    "quotes_df = pd.DataFrame(\n",
    "    {\"quote_speakers\": speakers\n",
    "     }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an array that has an ordered array of unique quote speakers\n",
    "unique_speakers = np.unique(np.array(speakers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Convert the unique speakers back into a list which can be loaded into a DataFrame\n",
    "unique_speakers_list = unique_speakers.tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe containing the unique speakers\n",
    "speakers_df = pd.DataFrame(\n",
    "    {\"speakers\": unique_speakers_list\n",
    "     }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find speakers in book descriptions and build a DataFrame of \"matches\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize empty list for storing IDs of the books and speakers who match\n",
    "matches_book_id_list = []\n",
    "matches_speaker_id_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through the speaker list and record each book description mentioning the speaker\n",
    "\n",
    "num_speakers = np.arange(0,len(unique_speakers_list))\n",
    "\n",
    "for i in num_speakers:\n",
    "\n",
    "    match_book_id_index = books_df[books_df['description'].str.contains(unique_speakers_list[i])].index\n",
    "    \n",
    "    if len(match_book_id_index) > 0:       \n",
    "        \n",
    "        for foo in match_book_id_index:\n",
    "            matches_book_id_list.append(foo)\n",
    "            matches_speaker_id_list.append(i)\n",
    "   \n",
    "    i = i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches_df = pd.DataFrame({\n",
    "    \"book_id\": matches_book_id_list,\n",
    "    \"speaker_id\": matches_speaker_id_list\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
