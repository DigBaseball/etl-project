{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dependencies\n",
    "\n",
    "from splinter import Browser\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "executable_path = {'executable_path': '/usr/local/bin/chromedriver'}\n",
    "browser = Browser('chrome', **executable_path, headless=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrape book data and load into a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the url to be the online bookstore, and open a scraping session\n",
    "url = 'http://books.toscrape.com/'\n",
    "browser.visit(url)\n",
    "browser_url = browser.url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize empty lists for storing scraped book titles, links, product descriptions, prices\n",
    "titles = []\n",
    "full_links = []\n",
    "product_descriptions = []\n",
    "prices = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/BootCamp/anaconda3/lib/python3.7/site-packages/splinter/driver/webdriver/__init__.py:536: FutureWarning: browser.find_link_by_text is deprecated. Use browser.links.find_by_text instead.\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping Complete\n"
     ]
    }
   ],
   "source": [
    "# Iterate through all pages\n",
    "for x in range(50):\n",
    "    # HTML object\n",
    "    html = browser.html\n",
    "    # Parse HTML with Beautiful Soup\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    # Retrieve all elements that contain book information\n",
    "    articles = soup.find_all('article', class_='product_pod')\n",
    "    browser_url = '/'.join(browser.url.rstrip('/').split('/')[:-1])\n",
    "\n",
    "    # Iterate through each book\n",
    "    for article in articles:\n",
    "        # Use Beautiful Soup's find() method to navigate and retrieve the anchor tag pertaining to each book\n",
    "        h3 = article.find('h3')\n",
    "        link = h3.find('a')\n",
    "\n",
    "        # Compile the book-specific web address, and handle the landing page's peculiar formatting\n",
    "        href = link['href']\n",
    "        if x == 0:\n",
    "            href = \"books.toscrape.com/\" + href\n",
    "        \n",
    "        # Complete the concatenation of the book page url\n",
    "        full_link = browser_url + \"/\" + href\n",
    "        full_links.append(full_link)\n",
    "\n",
    "        # Retrieve the title of the book and add it to our list of books\n",
    "        title = link['title']\n",
    "        titles.append(title)\n",
    "        \n",
    "\n",
    "    # Click the 'Next' button on each page, otherwise print that scraping is complete\n",
    "    try:\n",
    "        browser.click_link_by_text('next')\n",
    "          \n",
    "    except:\n",
    "        print(\"Scraping Complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Follow each book's link to grab the production description and price for each book\n",
    "for link in full_links:\n",
    "    browser.visit(link)\n",
    "    html = browser.html\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    \n",
    "    # Find and append the product description of the current book to our list\n",
    "    product_description = soup.find_all('p')[3].text\n",
    "    product_descriptions.append(product_description)\n",
    "        \n",
    "    # Find and append the float-formatted price of the current book to our list\n",
    "    price = float(soup.find_all('p', class_='price_color')[0].text.strip('£'))\n",
    "    prices.append(price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe containins the scraped book data (urls, titles, descriptions, and prices)\n",
    "books_df = pd.DataFrame(\n",
    "    {\"link\": full_links,\n",
    "     \"title\": titles,\n",
    "     \"description\": product_descriptions,\n",
    "     \"price\": prices\n",
    "     }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>link</th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>http://books.toscrape.com/catalogue/a-light-in...</td>\n",
       "      <td>A Light in the Attic</td>\n",
       "      <td>It's hard to imagine a world without A Light i...</td>\n",
       "      <td>51.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>http://books.toscrape.com/catalogue/tipping-th...</td>\n",
       "      <td>Tipping the Velvet</td>\n",
       "      <td>\"Erotic and absorbing...Written with starling ...</td>\n",
       "      <td>53.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>http://books.toscrape.com/catalogue/soumission...</td>\n",
       "      <td>Soumission</td>\n",
       "      <td>Dans une France assez proche de la nôtre, un h...</td>\n",
       "      <td>50.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>http://books.toscrape.com/catalogue/sharp-obje...</td>\n",
       "      <td>Sharp Objects</td>\n",
       "      <td>WICKED above her hipbone, GIRL across her hear...</td>\n",
       "      <td>47.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>http://books.toscrape.com/catalogue/sapiens-a-...</td>\n",
       "      <td>Sapiens: A Brief History of Humankind</td>\n",
       "      <td>From a renowned historian comes a groundbreaki...</td>\n",
       "      <td>54.23</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                link  \\\n",
       "0  http://books.toscrape.com/catalogue/a-light-in...   \n",
       "1  http://books.toscrape.com/catalogue/tipping-th...   \n",
       "2  http://books.toscrape.com/catalogue/soumission...   \n",
       "3  http://books.toscrape.com/catalogue/sharp-obje...   \n",
       "4  http://books.toscrape.com/catalogue/sapiens-a-...   \n",
       "\n",
       "                                   title  \\\n",
       "0                   A Light in the Attic   \n",
       "1                     Tipping the Velvet   \n",
       "2                             Soumission   \n",
       "3                          Sharp Objects   \n",
       "4  Sapiens: A Brief History of Humankind   \n",
       "\n",
       "                                         description  price  \n",
       "0  It's hard to imagine a world without A Light i...  51.77  \n",
       "1  \"Erotic and absorbing...Written with starling ...  53.74  \n",
       "2  Dans une France assez proche de la nôtre, un h...  50.10  \n",
       "3  WICKED above her hipbone, GIRL across her hear...  47.82  \n",
       "4  From a renowned historian comes a groundbreaki...  54.23  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preview the books DataFrame\n",
    "books_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrape quote data and load into DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the url to be the quotes website, and open a scraping session\n",
    "quotes_url = 'http://quotes.toscrape.com/'\n",
    "browser.visit(quotes_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty list for storing speaker names\n",
    "speakers = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/BootCamp/anaconda3/lib/python3.7/site-packages/splinter/driver/webdriver/__init__.py:528: FutureWarning: browser.find_link_by_partial_text is deprecated. Use browser.links.find_by_partial_text instead.\n",
      "  FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "# Iterate through each quote\n",
    "for x in range(1, 11):\n",
    "\n",
    "    # Parse HTML with Beautiful Soup\n",
    "    html = browser.html\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    quotes = soup.find_all('small', class_='author')\n",
    "\n",
    "    # Retrieve the name of the each quote speaker and add it to our list of speakers\n",
    "    for quote in quotes:\n",
    "        speakers.append(quote.text)\n",
    "\n",
    "    # Work through all ten pages of quotes by clicking 'Next' at the end of each page\n",
    "    if x != 10:\n",
    "        browser.click_link_by_partial_text('Next')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe containing the scraped quotes data (speakers)\n",
    "quotes_df = pd.DataFrame(\n",
    "    {\"quote_speaker\": speakers\n",
    "     }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an array that has an ordered array of unique quote speakers\n",
    "unique_speakers = np.unique(np.array(speakers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Convert the unique speakers back into a list which can be loaded into a DataFrame\n",
    "unique_speakers_list = unique_speakers.tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe containing the unique speakers\n",
    "speakers_df = pd.DataFrame(\n",
    "    {\"speaker\": unique_speakers_list\n",
    "     }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find speakers in book descriptions and build a DataFrame of \"matches\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize empty list for storing titles of the books and names speakers who match\n",
    "matches_book_title_list = []\n",
    "matches_speaker_name_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through the speaker list and record title of each book description mentioning the speaker names\n",
    "\n",
    "num_speakers = np.arange(0,len(unique_speakers_list))\n",
    "\n",
    "for i in num_speakers:\n",
    "\n",
    "    match_book_id_index = books_df[books_df['description'].str.contains(unique_speakers_list[i])].index\n",
    "    \n",
    "    if len(match_book_id_index) > 0:       \n",
    "        \n",
    "        for foo in match_book_id_index:\n",
    "            matches_book_title_list.append(books_df.iloc[foo,1])\n",
    "            matches_speaker_name_list.append(unique_speakers_list[i])\n",
    "   \n",
    "    i = i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame containing the book titles and speaker names where the product description matches\n",
    "matches_df = pd.DataFrame({\n",
    "    \"book_title\": matches_book_title_list,\n",
    "    \"speaker_name\": matches_speaker_name_list\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create database in Postgres using schema.sql"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This happens offline (i.e., using PgAdmin) prior to running the next code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create database connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a connection to the database we created offline\n",
    "connection_string = \"postgres:yX8nLv1jy7@localhost:5432/etl-project\"\n",
    "engine = create_engine(f'postgresql://{connection_string}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['quotes', 'books', 'matches', 'speakers']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Confirm tables\n",
    "engine.table_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load DataFrames into database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load books into Database\n",
    "books_df.to_sql(name='books', con=engine, if_exists='append', index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load quotes into Database\n",
    "quotes_df.to_sql(name='quotes', con=engine, if_exists='append', index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load speakers into Database\n",
    "speakers_df.to_sql(name='speakers', con=engine, if_exists='append', index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load books table back into DataFrames so we can grab database IDs of records of interest\n",
    "books_2_df = pd.read_sql(\"select * from books\", con=engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load speakers table back into DataFrames so we can grab database IDs of records of interest\n",
    "speakers_2_df = pd.read_sql(\"select * from speakers\", con=engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge matches DataFrame with books from DataBase\n",
    "matches_df_2 = matches_df.merge(books_2_df, left_on=\"book_title\", right_on=\"title\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge evolved matches DataFrame with speakers from DataBase\n",
    "matches_df_3 = matches_df_2.merge(speakers_2_df, left_on=\"speaker_name\", right_on=\"speaker\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a filtered matches dataframe from specific columns\n",
    "matches_cols = [\"id_x\", \"id_y\"]\n",
    "matches_transformed_df = matches_df_3[matches_cols].copy()\n",
    "\n",
    "# Rename the column headers\n",
    "matches_transformed_df = matches_transformed_df.rename(columns={\"id_x\": \"book_id\", \"id_y\": \"speaker_id\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load matches into Database\n",
    "matches_transformed_df.to_sql(name='matches', con=engine, if_exists='append', index=False) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
